{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Script for Olympic Athletes Dataset\n",
    "\n",
    "This notebook walks through the process of cleaning the \"bios.csv\" dataset, covering data inspection, transformation, and cleaning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data processing and visualization\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis, especially for working with DataFrames\n",
    "import numpy as np  # For numerical computations and handling arrays\n",
    "import seaborn as sns  # For data visualization, providing a high-level interface for drawing attractive graphs\n",
    "import matplotlib.pyplot as plt  # For plotting and visualizations, a versatile plotting library\n",
    "from sklearn.impute import SimpleImputer  # For handling missing data using the Simple Imputer with most frequent strategy\n",
    "import time  # Time library, used for handling time-related tasks\n",
    "import re  # Regular expressions library, used for string manipulation and pattern matching\n",
    "import os  # Provides functionalities to interact with the operating system, like reading file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "pwd = os.getcwd()  # Storing the current directory path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Display all columns in the DataFrame\n",
    "pd.set_option(\"display.max_rows\", None)     # Display all rows in the DataFrame\n",
    "\n",
    "# Load the uncleaned dataset into a pandas DataFrame\n",
    "dataset = pd.read_csv(pwd + \"/bios.csv\")  # Reading the CSV file into a DataFrame\n",
    "\n",
    "# Create a copy of the original dataset to work on\n",
    "df = dataset.copy()  # Making a copy to avoid altering the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Inspection\n",
    "We'll start by inspecting the dataset to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 random samples from the dataset\n",
    "df.sample(5)  # Randomly displaying 5 rows to get a glimpse of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "df.head()  # Showing the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the dataset\n",
    "df.tail()  # Showing the last few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of the dataset (number of rows and columns)\n",
    "df.shape  # Getting the dimensions of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the basic information (data types, non-null counts, etc.) of the dataset\n",
    "df.info()  # Displaying information about the DataFrame's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of missing values for each column\n",
    "df.isnull().sum()  # Counting missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() / df.shape[0] * 100  # Calculate the percentage of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of duplicate rows in the dataset\n",
    "df.duplicated().sum()  # Checking for duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows, if any, to avoid redundancy\n",
    "df = df.drop_duplicates()  # Removing any duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step of dropping unwanted columns for clean summary\n",
    "df = df.drop(columns=[\"Nick/petnames\",\"Title(s)\",\"Other names\",\n",
    "                      \"Nationality\",\"Original name\",\"Name order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the list of column names in the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging columns for better organization\n",
    "df = df[['athlete_id','Full name', 'Used name','Sex','Born',\n",
    "          'Died', 'NOC', 'Measurements', 'Roles','Affiliations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a single row from the rearranged DataFrame for a quick overview\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics\n",
    "\n",
    "Let's inspect summary statistics for both numeric and categorical columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all object type columns and display their value counts\n",
    "for i in df.select_dtypes(include=\"object\").columns:  # Iterating over object columns\n",
    "    print(f\"Value Counts for {i}:\\n\", df[i].value_counts())  # Displaying counts of unique values in each column\n",
    "    print(\"***\" * 10)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for object (categorical) columns\n",
    "df.describe(include=\"object\")  # Summary statistics for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning for Used Name Column\n",
    "\n",
    "In this section, we explore the \"Used name\" column, clean it by replacing unwanted characters, and then drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # Displays the column names of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the \"Used name\" column\n",
    "list(df[\"Used name\"].unique())  # Outputs a list of unique values in the \"Used name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for the \"Used name\" column\n",
    "df[\"Used name\"].describe()  # Provides summary statistics for the \"Used name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a new column \"name\" after the \"Used name\" column, replacing \"•\" with a space\n",
    "df.insert(loc=df.columns.get_loc(\"Used name\")+1, column=\"name\", value=df[\"Used name\"].str.replace(\"•\", \" \"))  \n",
    "# Adds a new \"name\" column, with '•' replaced by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 3 rows from the DataFrame\n",
    "df.sample(3)  # Displays 3 random rows from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Full name\" and \"Used name\" columns from the DataFrame\n",
    "df = df.drop(columns=[\"Full name\", \"Used name\"])  # Removes the \"Full name\" and \"Used name\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display another random sample of 3 rows from the DataFrame\n",
    "df.sample(3)  # Displays 3 random rows from the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring the Sex Column\n",
    "\n",
    "In this section, we analyze the \"Sex\" column to understand its unique values, descriptive statistics, and distribution through visual representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in the \"Sex\" column\n",
    "df[\"Sex\"].unique()  # Outputs an array of unique values in the \"Sex\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for the \"Sex\" column\n",
    "df[\"Sex\"].describe()  # Provides summary statistics for the \"Sex\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the \"Sex\" column\n",
    "df[\"Sex\"].value_counts()  # Returns a Series with counts of unique values in the \"Sex\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of the value counts for the \"Sex\" column\n",
    "df[\"Sex\"].value_counts().plot.bar()  # Generates a bar plot to visualize the distribution of values in the \"Sex\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Cleaning and Transformation for the Born Column\n",
    "\n",
    "In this section, we clean and transform the \"Born\" column to extract birth date and location details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List unique values in the \"Born\" column\n",
    "list(df[\"Born\"].unique())  # Outputs a list of unique values in the \"Born\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in the \"Born\" column\n",
    "df[\"Born\"].value_counts()  # Returns a Series with counts of unique values in the \"Born\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column names for birth date and location\n",
    "new_cols1 = [\"birth_date\", \"birth_location\"]  # Names for the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Born' column into two new columns based on the word 'in'\n",
    "new_data1 = df[\"Born\"].str.split(\"in\", expand=True)  # Splits the column into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace month names with their respective numeric values\n",
    "new_data1[0] = new_data1[0].replace({\"January\":\"-01-\",\"February\":\"-02-\",\"March\":\"-03-\",\"April\":\"-04-\",\n",
    "                                      \"May\":\"-05-\",\"June\":\"-06-\",\"July\":\"-07-\",\"August\":\"-08-\",\n",
    "                                      \"September\":\"-09-\",\"October\":\"-10-\",\"November\":\"-11-\",\"December\":\"-12-\"}, regex=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading and trailing spaces from birth_date and birth_location columns\n",
    "new_data1[0] = new_data1[0].str.strip().replace(\" \", \"\", regex=False)  # Cleans the birth_date\n",
    "new_data1[1] = new_data1[1].str.strip().replace(\" \", \"\", regex=False)  # Cleans the birth_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the new columns into the DataFrame\n",
    "for i in range(len(new_cols1)):\n",
    "    df.insert(loc=df.columns.get_loc(\"Born\") + 1 + i, column=new_cols1[i], value=new_data1[i])  # Adds new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 3 rows from the updated DataFrame\n",
    "df.sample(3)  # Outputs a random sample of 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces within the 'birth_date' strings\n",
    "df[\"birth_date\"] = df[\"birth_date\"].str.replace(\" \", \"\", regex=False)  # Cleans the birth_date values\n",
    "\n",
    "# Convert 'birth_date' to datetime format after cleaning\n",
    "df[\"birth_date\"] = pd.to_datetime(df[\"birth_date\"], format=\"%d-%m-%Y\", errors='coerce')  # Converts to datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display another random sample of 3 rows from the updated DataFrame\n",
    "df.sample(3)  # Outputs a random sample of 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces within the 'birth_location' strings\n",
    "df[\"birth_location\"] = df[\"birth_location\"].str.replace(\" \", \"\", regex=False)  # Cleans the birth_location values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column names for City and Region_country\n",
    "new_cols2 = [\"City\", \"Region_country\"]  # Names for the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'birth_location' column into two new columns based on the comma\n",
    "new_data2 = df[\"birth_location\"].str.split(\",\", expand=True)  # Splits the column into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the new columns into the DataFrame\n",
    "for i in range(len(new_cols2)):\n",
    "    df.insert(loc=df.columns.get_loc(\"birth_location\") + 1 + i, column=new_cols2[i], value=new_data2[i])  # Adds new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample from the updated DataFrame\n",
    "df.sample()  # Outputs a random sample of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column names for Region and Country\n",
    "new_cols3 = [\"Region\", \"Country\"]  # Names for the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Region_country' column into two new columns based on the opening parenthesis\n",
    "new_data3 = df[\"Region_country\"].str.split(\"(\", expand=True)  # Splits the column into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the new columns into the DataFrame\n",
    "for i in range(len(new_cols3)):\n",
    "    df.insert(loc=df.columns.get_loc(\"Region_country\") + 1 + i, column=new_cols3[i], value=new_data3[i])  # Adds new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample from the updated DataFrame\n",
    "df.sample()  # Outputs a random sample of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the closing parenthesis ')' from the 'Country' column\n",
    "df[\"Country\"] = df[\"Country\"].str.replace(\")\", \"\", regex=False)  # Cleans the Country values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample from the updated DataFrame\n",
    "df.sample()  # Outputs a random sample of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Born', 'birth_location', 'Region_country', and 'Died' columns from the DataFrame\n",
    "df = df.drop(columns=[\"Born\", \"birth_location\", \"Region_country\", \"Died\"])  # Removes specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample from the updated DataFrame\n",
    "df.sample()  # Outputs a random sample of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NOC Column Inspection and Cleaning\n",
    "\n",
    "In this section, we inspect the \"NOC\" (National Olympic Committee) column for unique values, descriptions, and missing values, then clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in the \"NOC\" column\n",
    "df[\"NOC\"].unique()  # Outputs a list of unique values in the \"NOC\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for the \"NOC\" column\n",
    "df[\"NOC\"].describe()  # Returns descriptive statistics for the \"NOC\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in the \"NOC\" column\n",
    "df[\"NOC\"].value_counts()  # Returns a Series with counts of unique values in the \"NOC\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where \"NOC\" is NaN (missing)\n",
    "nan_in_noc = df[\"NOC\"].isna()  # Creates a boolean Series indicating missing values\n",
    "df[nan_in_noc]  # Outputs the rows where the \"NOC\" column has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where \"NOC\" is NaN\n",
    "df.dropna(subset=[\"NOC\"], inplace=True)  # Removes rows with NaN in the \"NOC\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Measurements Column Inspection and Cleaning\n",
    "\n",
    "In this section, we inspect the \"Measurements\" column for unique values, descriptions, and then clean the data to extract height and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in the \"Measurements\" column\n",
    "df[\"Measurements\"].unique()  # Outputs unique values found in the \"Measurements\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in the \"Measurements\" column\n",
    "df[\"Measurements\"].value_counts()  # Returns a Series with counts of unique values in the \"Measurements\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for the \"Measurements\" column\n",
    "df[\"Measurements\"].describe()  # Returns descriptive statistics for the \"Measurements\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new columns for height and weight\n",
    "new_cols4 = [\"height_cm\", \"weight_kg\"]  # List of new column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'Measurements' column into two new columns; will result in NaN if there's no second part\n",
    "new_data4 = df[\"Measurements\"].str.split(\"/\", expand=True)  # Split the measurements into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip any whitespace from the split columns\n",
    "new_data4 = new_data4.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)  # Remove whitespace\n",
    "\n",
    "# Initialize new columns in the DataFrame\n",
    "df[\"height_cm\"] = None  # Create a column for height\n",
    "df[\"weight_kg\"] = None  # Create a column for weight\n",
    "\n",
    "# Handle cases where there's only one value (either height or weight)\n",
    "for i, row in new_data4.iterrows():  # Iterate over rows of new data\n",
    "    if pd.isna(row[1]):  # If the second column is NaN (indicating a single value)\n",
    "        if pd.notna(row[0]):  # Check if the single value is valid\n",
    "            if \"cm\" in row[0]:  # Check if the value contains \"cm\"\n",
    "                df.at[i, \"height_cm\"] = row[0].replace(\" cm\", \"\")  # Store height\n",
    "            elif \"kg\" in row[0]:  # Check if the value contains \"kg\"\n",
    "                df.at[i, \"weight_kg\"] = row[0].replace(\" kg\", \"\")  # Store weight\n",
    "    else:\n",
    "        # If both height and weight are present, assign them to the respective columns\n",
    "        if pd.notna(row[0]):  # Check if height is valid\n",
    "            df.at[i, \"height_cm\"] = row[0].replace(\" cm\", \"\")  # Store height\n",
    "        if pd.notna(row[1]):  # Check if weight is valid\n",
    "            df.at[i, \"weight_kg\"] = row[1].replace(\" kg\", \"\")  # Store weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"cm\" and \"kg\" in the split data\n",
    "new_data4 = new_data4.replace({\"cm\": \"\", \"kg\": \"\"}, regex=True)  # Remove units from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric values\n",
    "df[\"height_cm\"] = pd.to_numeric(df[\"height_cm\"], errors=\"coerce\")  # Convert height to numeric, coercing errors\n",
    "df[\"weight_kg\"] = pd.to_numeric(df[\"weight_kg\"], errors=\"coerce\")  # Convert weight to numeric, coercing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of the DataFrame after transformations\n",
    "df.sample()  # Outputs a random sample of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original \"Measurements\" column as it is no longer needed\n",
    "df = df.drop(columns=[\"Measurements\"])  # Remove the \"Measurements\" column from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the DataFrame to show only relevant columns\n",
    "df = df[['athlete_id', 'name', 'Sex', 'birth_date', 'City', 'Region', 'Country',\n",
    "       'NOC','height_cm','weight_kg', 'Roles', 'Affiliations']]  # Select specific columns to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of the DataFrame after rearranging columns\n",
    "df.sample()  # Outputs a random sample of the DataFrame after column rearrangement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Handling Missing Values and Final Data Cleanup\n",
    "\n",
    "In this section, we check for missing values in the DataFrame, fill missing entries, and apply final transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df.isna().sum()  # Returns the count of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the names of columns in the DataFrame\n",
    "df.columns  # Outputs the list of column names in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in specific columns with \"Unknown\"\n",
    "df[[\"City\", \"Region\", \"Country\", \"Affiliations\"]] = df[[\"City\", \"Region\", \"Country\", \"Affiliations\"]].fillna(\"Unknown\")  # Replace NaN with \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values again after filling\n",
    "df.isna().sum()  # Returns the count of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the DataFrame, including data types and non-null counts\n",
    "df.info()  # Outputs a concise summary of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SimpleImputer with the strategy set to \"most_frequent\"\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")  # Create an imputer for filling missing values with the most frequent value\n",
    "\n",
    "# Iterate through numeric columns and fill missing values\n",
    "for i in df.select_dtypes(include=\"number\").columns:  # Loop through numeric columns\n",
    "    df[i] = imputer.fit_transform(df[[i]])  # Fill missing values in each numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing birth_date with a constant value\n",
    "df[\"birth_date\"].fillna(pd.Timestamp(\"1975-01-01\"), inplace=True)  # Replace NaN in 'birth_date' with a specified date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the height and weight columns to one decimal place\n",
    "df[\"height_cm\"] = df[\"height_cm\"].round(1)  # Round height to one decimal place\n",
    "df[\"weight_kg\"] = df[\"weight_kg\"].round(1)  # Round weight to one decimal place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining missing values in the DataFrame\n",
    "df.isna().sum()  # Returns the count of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 3 rows from the DataFrame\n",
    "df.sample(3)  # Outputs a random sample of 3 rows from the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Output\n",
    "In this section, we'll create a copy of the cleaned DataFrame and save it as a CSV file named `cleaned_bios.csv`. This file will be stored in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the cleaned DataFrame\n",
    "output = df.copy()  # Making a copy of the DataFrame to preserve the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "output.to_csv(pwd + \"/cleaned_bios.csv\", index=False)  # Saving the DataFrame to a CSV without the index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
